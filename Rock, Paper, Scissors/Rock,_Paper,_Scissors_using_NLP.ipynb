{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rock, Paper, Scissors using NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShpiSKboOUz9"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import time\r\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8gO-CUZOZ2o",
        "outputId": "dda4a619-f78f-47bf-acaa-318428c9cf76"
      },
      "source": [
        "# Get the data from Google Drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "path_to_file = \"/content/drive/My Drive/train.txt\"\r\n",
        "\r\n",
        "# Read, then decode for py2 compat.\r\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n",
        "# length of text is the number of characters in it\r\n",
        "print('Length of text: {} characters'.format(len(text)))\r\n",
        "\r\n",
        "# The unique characters in the file\r\n",
        "vocab = sorted(set(text))\r\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Length of text: 111540 characters\n",
            "3 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6muQfw2oQuTz",
        "outputId": "b61eab6d-be53-45a1-9a9e-dedf44e4c675"
      },
      "source": [
        "# Vectorize the text\r\n",
        "# Creating a mapping from unique characters to indices\r\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\r\n",
        "idx2char = np.array(vocab)\r\n",
        "\r\n",
        "text_as_int = np.array([char2idx[c] for c in text])\r\n",
        "\r\n",
        "print('{')\r\n",
        "for char,_ in zip(char2idx, range(20)):\r\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\r\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  'P' :   0,\n",
            "  'R' :   1,\n",
            "  'S' :   2,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRvyH_mDRR4y",
        "outputId": "1d8d0bcd-f345-4320-9eaf-ff7ff0675a17"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\r\n",
        "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'RRRRRPPPPPSSS' ---- characters mapped to int ---- > [1 1 1 1 1 0 0 0 0 0 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ddG3L3kRXuf",
        "outputId": "170d8dda-e760-449e-a289-3efee7bf2a62"
      },
      "source": [
        "# The maximum length sentence you want for a single input in characters\r\n",
        "seq_length = 5\r\n",
        "examples_per_epoch = len(text)//(seq_length+1)\r\n",
        "\r\n",
        "# Create training examples / targets\r\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\r\n",
        "\r\n",
        "for i in char_dataset.take(5):\r\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R\n",
            "R\n",
            "R\n",
            "R\n",
            "R\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAMlOHPARvIA",
        "outputId": "595990ec-3718-407c-e3ba-870fb35cd55e"
      },
      "source": [
        "# The batch method lets us easily convert these individual characters to sequences of the desired size.\r\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\r\n",
        "\r\n",
        "for item in sequences.take(5):\r\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'RRRRRP'\n",
            "'PPPPSS'\n",
            "'SSSRRR'\n",
            "'RRPPPP'\n",
            "'PSSSSS'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0ACUq3kR3UZ",
        "outputId": "7509f6ae-2c27-41ee-f2e8-ad9ab5d41ec8"
      },
      "source": [
        "# For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch\r\n",
        "def split_input_target(chunk):\r\n",
        "    input_text = chunk[:-1]\r\n",
        "    target_text = chunk[1:]\r\n",
        "    return input_text, target_text\r\n",
        "\r\n",
        "dataset = sequences.map(split_input_target)\r\n",
        "\r\n",
        "# Print the first example input and target values\r\n",
        "for input_example, target_example in  dataset.take(1):\r\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\r\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'RRRRR'\n",
            "Target data: 'RRRRP'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcDrnXjpW0Nt",
        "outputId": "af3b82c0-5fa5-441b-e327-6119499019ed"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\r\n",
        "    print(\"Step {:4d}\".format(i))\r\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\r\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 1 ('R')\n",
            "  expected output: 1 ('R')\n",
            "Step    1\n",
            "  input: 1 ('R')\n",
            "  expected output: 1 ('R')\n",
            "Step    2\n",
            "  input: 1 ('R')\n",
            "  expected output: 1 ('R')\n",
            "Step    3\n",
            "  input: 1 ('R')\n",
            "  expected output: 1 ('R')\n",
            "Step    4\n",
            "  input: 1 ('R')\n",
            "  expected output: 0 ('P')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn1OR0u4W4h1",
        "outputId": "919acb95-0db2-46cb-a36f-8f04e6e67820"
      },
      "source": [
        "# Create training batches\r\n",
        "# Batch size\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "# Buffer size to shuffle the dataset\r\n",
        "# (TF data is designed to work with possibly infinite sequences,\r\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\r\n",
        "# it maintains a buffer in which it shuffles elements).\r\n",
        "BUFFER_SIZE = 10000\r\n",
        "\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "\r\n",
        "dataset"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 5), (64, 5)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dgZsrFMXM4V"
      },
      "source": [
        "# Build The Model\r\n",
        "# Length of the vocabulary in chars\r\n",
        "vocab_size = len(vocab)\r\n",
        "\r\n",
        "# The embedding dimension\r\n",
        "embedding_dim = 256\r\n",
        "\r\n",
        "# Number of RNN units\r\n",
        "rnn_units = 1024 * 4\r\n",
        "\r\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n",
        "    model = tf.keras.Sequential([\r\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\r\n",
        "                                  batch_input_shape=[batch_size, None]),\r\n",
        "        tf.keras.layers.LSTM(rnn_units,\r\n",
        "                            return_sequences=True,\r\n",
        "                            stateful=True, # batch_input_shape=(batch_size, timesteps, data_dim)\r\n",
        "                            recurrent_initializer='glorot_uniform'),\r\n",
        "        tf.keras.layers.Dense(vocab_size)\r\n",
        "    ])\r\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX9wcFj_XSFC",
        "outputId": "b69fcb68-a9cf-4f63-f8be-30b1aee0bfd3"
      },
      "source": [
        "model = build_model(\r\n",
        "    vocab_size=len(vocab),\r\n",
        "    embedding_dim=embedding_dim,\r\n",
        "    rnn_units=rnn_units,\r\n",
        "    batch_size=BATCH_SIZE)\r\n",
        "\r\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\r\n",
        "    example_batch_predictions = model(input_example_batch)\r\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 5, 3) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T59qJ4JdXfg7",
        "outputId": "b98d62f3-6f7c-4a3e-e845-9a211b23e578"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           768       \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 4096)          71319552  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 3)             12291     \n",
            "=================================================================\n",
            "Total params: 71,332,611\n",
            "Trainable params: 71,332,611\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxgdgzHAXlb9",
        "outputId": "09da68c7-1bc6-4d2a-801d-f43a45280d74"
      },
      "source": [
        "# To get actual predictions from the model you need to sample from the output distribution, to get actual character indices\r\n",
        "# This distribution is defined by the logits over the character vocabulary\r\n",
        "\r\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\r\n",
        "\r\n",
        "sampled_indices\r\n",
        "\r\n",
        "print(\"Input: \\n\", repr(\"\".join([idx2char[i] for i in input_example_batch[0]])))\r\n",
        "print()\r\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'PSSSS'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'RPPSR'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiTfffRvXvg7",
        "outputId": "2a88a189-bf49-4752-bee3-578826c12be5"
      },
      "source": [
        "# Train the model\r\n",
        "def loss(labels, logits):\r\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\r\n",
        "\r\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\r\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 5, 3)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       1.098037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSwKDZVkYws1"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMXSSCr-Yyz_"
      },
      "source": [
        "# Directory where the checkpoints will be saved\r\n",
        "checkpoint_dir = '/content/drive/My Drive/training_checkpoints'\r\n",
        "# Name of the checkpoint files\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "\r\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix,\r\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw_kFxO-Y4bZ",
        "outputId": "ecec9af0-e87f-4e4c-d174-deb882f24acc"
      },
      "source": [
        "# Execute the training\r\n",
        "EPOCHS = 10\r\n",
        "\r\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "290/290 [==============================] - 21s 66ms/step - loss: 0.4616 - accuracy: 0.8028\n",
            "Epoch 2/10\n",
            "290/290 [==============================] - 20s 67ms/step - loss: 0.1667 - accuracy: 0.9128\n",
            "Epoch 3/10\n",
            "290/290 [==============================] - 20s 68ms/step - loss: 0.1169 - accuracy: 0.9200\n",
            "Epoch 4/10\n",
            "290/290 [==============================] - 20s 69ms/step - loss: 0.1154 - accuracy: 0.9202\n",
            "Epoch 5/10\n",
            "290/290 [==============================] - 21s 70ms/step - loss: 0.1140 - accuracy: 0.9197\n",
            "Epoch 6/10\n",
            "290/290 [==============================] - 21s 71ms/step - loss: 0.1142 - accuracy: 0.9190\n",
            "Epoch 7/10\n",
            "290/290 [==============================] - 21s 71ms/step - loss: 0.1147 - accuracy: 0.9194\n",
            "Epoch 8/10\n",
            "290/290 [==============================] - 21s 71ms/step - loss: 0.1128 - accuracy: 0.9207\n",
            "Epoch 9/10\n",
            "290/290 [==============================] - 21s 71ms/step - loss: 0.1285 - accuracy: 0.9161\n",
            "Epoch 10/10\n",
            "290/290 [==============================] - 21s 72ms/step - loss: 0.1426 - accuracy: 0.9141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJcROKE6llgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b108308c-1ee7-4dd4-fcaf-b7254cdc8038"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\r\n",
        "\r\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\r\n",
        "\r\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n",
        "\r\n",
        "model.build(tf.TensorShape([1, None]))\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            768       \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 4096)           71319552  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 3)              12291     \n",
            "=================================================================\n",
            "Total params: 71,332,611\n",
            "Trainable params: 71,332,611\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGp5fuuERzI9"
      },
      "source": [
        "def load_model(checkpoint_dir, vocab_size, embedding_dim, rnn_units):\r\n",
        "\r\n",
        "  model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\r\n",
        "\r\n",
        "  model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n",
        "\r\n",
        "  model.build(tf.TensorShape([1, None]))\r\n",
        "\r\n",
        "  model.summary()\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfL9DoV0XQHD"
      },
      "source": [
        "def generate_text(model, start_string, test_text, help = False, num_generate = 100):\r\n",
        "    # Evaluation step (generating text using the learned model)\r\n",
        "\r\n",
        "    # Converting our start string to numbers (vectorizing)\r\n",
        "    input_eval = [char2idx[s] for s in start_string]\r\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\r\n",
        "\r\n",
        "    # Empty string to store our results\r\n",
        "    text_generated = []\r\n",
        "\r\n",
        "    # Low temperature results in more predictable text.\r\n",
        "    # Higher temperature results in more surprising text.\r\n",
        "    # Experiment to find the best setting.\r\n",
        "    temperature = 1.0\r\n",
        "\r\n",
        "    # Count number of matches\r\n",
        "    count = 0\r\n",
        "\r\n",
        "    # Here batch size == 1\r\n",
        "    model.reset_states()\r\n",
        "    for i in range(num_generate):\r\n",
        "        predictions = model(input_eval)\r\n",
        "        # remove the batch dimension\r\n",
        "        predictions = tf.squeeze(predictions, 0)\r\n",
        "\r\n",
        "        # using a categorical distribution to predict the character returned by the model\r\n",
        "        predictions = predictions / temperature\r\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n",
        "\r\n",
        "        # Pass the predicted character as the next input to the model\r\n",
        "        # along with the previous hidden state\r\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\r\n",
        "\r\n",
        "        next_char = idx2char[predicted_id]\r\n",
        "\r\n",
        "        # If network predicted correctly\r\n",
        "        if next_char == test_text[i]:\r\n",
        "          count += 1\r\n",
        "\r\n",
        "        # If with help, set next char the real one from test text\r\n",
        "        if help:\r\n",
        "          next_char = test_text[i]\r\n",
        "\r\n",
        "        text_generated.append(next_char)\r\n",
        "\r\n",
        "    return count"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLyq4TQxVfKP",
        "outputId": "df2af24b-816f-44da-ee2f-01e2f6ea5150"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "\r\n",
        "# Load model\r\n",
        "model = load_model(checkpoint_dir, vocab_size, embedding_dim, rnn_units)\r\n",
        "\r\n",
        "# Testing\r\n",
        "path_to_file_test = \"/content/drive/My Drive/test.txt\"\r\n",
        "\r\n",
        "# Read, then decode for py2 compat.\r\n",
        "test_text = open(path_to_file_test, 'rb').read().decode(encoding='utf-8')\r\n",
        "# length of text is the number of characters in it\r\n",
        "print('Length of text: {} characters'.format(len(test_text)))\r\n",
        "\r\n",
        "def test_model(model, test_text, help = False):\r\n",
        "  print(\"Helping network to predict :\", help)\r\n",
        "  print(generate_text(model, start_string = str(test_text[0:seq_length]), test_text = test_text[seq_length:], help = help, num_generate = len(test_text) - seq_length))\r\n",
        "\r\n",
        "test_model(model, test_text, True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            768       \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (1, None, 4096)           71319552  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 3)              12291     \n",
            "=================================================================\n",
            "Total params: 71,332,611\n",
            "Trainable params: 71,332,611\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Length of text: 1110 characters\n",
            "Helping network to predict : True\n",
            "403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5UTqVeJSoqy"
      },
      "source": [
        "# Results\r\n",
        "\r\n",
        "# Trained on file generated with step 5 (RRRRRPPPPPSSSSS), accuracy 0.9 (with seq_length = 5)\r\n",
        "\r\n",
        "# Testing on the file where only R appears -> 403/1000\r\n",
        "# Testing on file generated with step 5 (RRRRRPPPPPSSSSS) as data for training -> 853/1000\r\n",
        "\r\n",
        "# Conclusion\r\n",
        "# If there IS a pattern in training set, with a good seq_length parameter (need to find it experimentally) its possible to learn pattern of player\r\n",
        "# Having pattern of player after predicting her/his move we can \"show\" the opposite figure and win\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}